{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e1153fd-06f3-4ce3-a568-3c737cb73b04",
   "metadata": {},
   "source": [
    "### Variable Types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd25507-315c-4be3-a0d0-fa41d6fb4569",
   "metadata": {},
   "source": [
    "Quantitative. e.g., glucose prediction.\n",
    "\n",
    "Qualitative. e.g., handwritten digits, denoted by $\\mathcal{G} = \\{0, 1, \\dots, 9\\}$.\n",
    "\n",
    "When we're predicting a quantitative variable, it's a regression task, while predicting a qualitative variable is a classificaiton task. Some methods of prediction are good for one or the other task, while some can be good for both.\n",
    "\n",
    "Some variable can be $\\textit{ordered categorical}$, such as $\\textit{small, medium}$ and $\\textit{large}$.\n",
    "\n",
    "Qualitative variables are typically represented numerically by codes. The most useful and commonly used coding is via $\\textit{dummy variables}$. Here a $K$-level qualitative variable is represented by a vector of $K$ binary variables or bits, only one of which is \"on\" at a time. Although more compact coding schemes are possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3015f6-0bad-442a-b37a-43bd23d7641f",
   "metadata": {},
   "source": [
    "### Terminology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d49ab94-ff39-48f8-ac8f-dfb3256a7619",
   "metadata": {},
   "source": [
    "$X$: input variable\n",
    "\n",
    "$X_j$: individual component of vector $X$\n",
    "\n",
    "$Y$: quantitative output\n",
    "\n",
    "$G$: qualitative output\n",
    "\n",
    "$x_i$: observed value of $X$, (where $x_i$ can be a scalar or a vector)\n",
    "\n",
    "$\\textbf{X}$: a matrix, e.g., a set of $N$ input $p$-vectors $x_i, i = 1, \\dots, N$ would be represented by $N \\times p$ matrix $\\textbf{X}$\n",
    "\n",
    "Vectors will not be bold, except when they have $N$ components. $p$-vector of inputs is denoted by $x_i$ for $i$th observarion, but for $N$-vector, $\\textbf{x}_j$ denotes all the observations on variable $X_j$\n",
    "\n",
    "\\begin{bmatrix}\n",
    "1 & 2 & 3 \\\\\n",
    "4 & 5 & 6 \\\\\n",
    "7 & 8 & 9 \\\\\n",
    "\\end{bmatrix}\n",
    "\n",
    "Here, $p$-vector of inputs is denoted by $x_1$ is $\\{1, 5, 7\\}$ and $N$-vector, $\\textbf{x}_1$ is $\\{1, 2, 3\\}$\n",
    "\n",
    "Since all vectors are assumed to be column vectors, the $i$th row of $\\textbf{X}$ is $x^{T}_{i}$, the vector transpose of $x_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef2c100-07c4-4cfb-826c-7bdc114ad712",
   "metadata": {},
   "source": [
    "### Learning task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a321698f-5eff-4424-9427-8d6679c543b3",
   "metadata": {},
   "source": [
    "given the value of an input vector $X$, make a good prediction of the output $Y$, denoted by $\\hat{Y}$ (pronounced as \"y-hat\").\n",
    "\n",
    "If $Y$ takes values in $\\mathbb{R}$ then so should $\\hat{Y}$; likewise for categorical outputs, $\\hat{G}$ should take values in the same set $\\mathcal{G}$ associated with $G$.\n",
    "\n",
    "For a two-class $G$, one approach is to denote the binary coded target as $Y$, and then treat it as a quantitative output.\n",
    "\n",
    "The predictions $\\hat{Y}$ will typically lie in $\\left[0, 1\\right]$, and we can assign to $\\hat{G}$ the class label according to whether $\\hat{y}$ > 0.5. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e73f6a1-e2a5-42d4-9d8a-d3ecf04a37a7",
   "metadata": {},
   "source": [
    "### Least Squares and Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe9a549-7be8-4041-913c-19301517fb4d",
   "metadata": {},
   "source": [
    "The linear model fit by Least Squares makes huge assumptions about structure and yields stable but possibly inaccurate predictions.\n",
    "\n",
    "The method of $k$-nearest neighbors makes very mild structural assumptions: its predictions are often accurate but can be unstable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd7d429-b61f-41b9-92fa-ed19e97bc833",
   "metadata": {},
   "source": [
    "### Linear Models and Least Squares"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a89ebfd-9aed-4b81-9264-97ad589c6ad2",
   "metadata": {},
   "source": [
    "Given a vector of inputs $X^T$ = $(X_1, X_2, \\dots , X_p)$, we predict the output $Y$ via the model:\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{Y} = \\hat{\\beta}_0 + \\sum_{i=1}^{p}X_{j}\\hat{\\beta}_{j}\n",
    "\\end{equation}\n",
    "\n",
    "The term $\\hat{\\beta}_0$ is the intercept or bias. $X^T$ denotes vector or matrix transpose ($X$ being a column vector).\n",
    "\n",
    "Including a constant variable 1 in $X$ and $\\hat{\\beta}_0$ in the vector of coefficients $\\hat{\\beta}$, We can represent the model in vector form as an inner product:\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{Y} = X^T\\hat{\\beta}\n",
    "\\end{equation}\n",
    "\n",
    "we pick the coefficients $\\beta$ to minimize the residual sum of squares\n",
    "\n",
    "\\begin{equation}\n",
    "RSS(\\beta) = \\sum_{i=1}^{N}(y_i - x^T_i\\beta)^{2}\n",
    "\\end{equation}\n",
    "\n",
    "In matrix notation\n",
    "\n",
    "\\begin{equation}\n",
    "RSS(\\beta) = (y - \\textbf{X}\\beta)^{T}(y - \\textbf{X}\\beta)\n",
    "\\end{equation}\n",
    "\n",
    "Differentiating w.r.t. $\\beta$ we get the normal equations\n",
    "\n",
    "\\begin{equation}\n",
    "\\textbf{X}^{T}(y - \\textbf{X}\\beta) = 0\n",
    "\\end{equation}\n",
    "\n",
    "If $X^TX$ is nonsingular, solving this equation\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{\\beta} = (\\textbf{X}^{T}\\textbf{X})^{-1}\\textbf{X}^Ty\n",
    "\\end{equation}\n",
    "\n",
    "and the fitted value at the $i$th input $x_i$ is $\\hat{y}_i = x^T_i \\hat{\\beta}$. At an arbitrary input $x_0$ the prediction is $\\hat{y}_0 = x^T_0 \\hat{\\beta}$\n",
    "\n",
    "Here we are modeling a single output, so $\\hat{Y}$ is a scalar; in general $\\hat{Y}$ can be a $K$–vector, in which case $\\beta$ would be a $p \\times K$ matrix of coefficients.\n",
    "\n",
    "In the ($p$ + 1)-dimensional input–output space, ($X$,  $\\hat{Y}$) represents a hyperplane.\n",
    "\n",
    "If the constant is included in $X$, then the hyperplane includes the origin and is a subspace; if not, it is an affine set cutting the $Y$-axis at the point (0, $\\hat{\\beta}_0$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0ad241-8c8e-4de9-8382-011eaab60d9a",
   "metadata": {},
   "source": [
    "### Nearest-Neighbor Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37fab063-9430-44c3-be03-f6e2237c3ce5",
   "metadata": {},
   "source": [
    "$k$-nearest neighbor fit for $\\hat{Y}$\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{Y}(x) = \\frac{1}{k} \\sum_{x_i \\in N_k(x)} y_i\n",
    "\\end{equation}\n",
    "\n",
    "where $N_k(x)$ is the neighborhood of $x$ defined by the $k$ closest points $x_i$ in the training sample.\n",
    "\n",
    "This closeness can be Euclidean distance.\n",
    "\n",
    "So, we find the $k$ observations with $x_i$ closest to $x$ in input space, and average their responses.\n",
    "\n",
    "Just like we had $p$ parameters in Linear Model, we have $k$ parameters in Nearest-Neighbour."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79777b69-1b61-4a7b-8d6b-98705a49181b",
   "metadata": {},
   "source": [
    "### Statistical Decision Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2a8599-2c81-4c34-ac6f-270b48a39b57",
   "metadata": {},
   "source": [
    "We seek a function $f(X)$ for predicting $Y$ given values of the input $X$. \n",
    "\n",
    "Let $X \\in \\mathbb{R}^p$ denote a real valued random input vector, and $Y ∈ \\mathbb{R}$ a real valued random output variable, with joint distribution $Pr(X, Y )$.\n",
    "\n",
    "This theory requires a loss function $L(Y, f(X))$ for penalizing errors in prediction, and by far the most common and convenient is squared error loss: $L(Y, f(X)) = (Y − f(X))^2$.\n",
    "\n",
    "We denote the Expected (squared) prediction error given below,\n",
    "\n",
    "\\begin{align*}\n",
    "    EPE(f) &= E(Y − f(X))^2 \\\\\n",
    "           &= \\int \\left[y − f(x)\\right]^2Pr(dx, dy) \\\\\n",
    "           &= E_XE_{Y|X}(\\left[Y − f(X)\\right]^2|X), \\text{conditioning w.r.t. X}\n",
    "\\end{align*}\n",
    "\n",
    "minimize EPE pointwise\n",
    "\\begin{align*}\n",
    "    f(x) = argmin_cE_{Y|X}(\\left[Y − c\\right]^2|X = x) \n",
    "\\end{align*}\n",
    "\n",
    "solving this\n",
    "\\begin{align*}\n",
    "    f(x) = E(Y|X = x) \n",
    "\\end{align*}\n",
    "\n",
    "Thus the best prediction of $Y$ at any point $X = x$ is the conditional mean, when best is measured by average squared error.\n",
    "\n",
    "What do we do when the output is a categorical variable $G$? \n",
    "\n",
    "We pick a different loss function.\n",
    "\\begin{align*}\n",
    "    EPE &= E\\left[L(G, \\hat{G}(X))\\right] \\\\\n",
    "        &= E_X \\sum_{k=1}^{K}L\\left[Gk, \\hat{G}(X)\\right]Pr(\\mathcal{G}_k|X), \\text{conditioning w.r.t. X}\n",
    "\\end{align*}\n",
    "\n",
    "minimize EPE pointwise\n",
    "\\begin{align*}\n",
    "    \\hat{G}(x) = argmin_{g \\in \\mathcal{G}}\\sum_{k=1}^{K}L(\\mathcal{G}_k, g)Pr(\\mathcal{G}_k|X=x) \n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28557913-1889-44be-8225-b1fc28dca582",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
